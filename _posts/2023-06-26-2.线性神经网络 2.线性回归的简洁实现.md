---
title: 【深度学习基础】2.线性神经网络 2.线性回归的简洁实现
date: 2023-06-26 00:00:01 +0800
img_path: /assets/img/md
categories: [记录, 深度学习基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
math: true
---
## 1. 数据集生成

全文依托于李沐老师的《动手学深度学习》一书，旨在对学习过程中的一些知识点作一点个人的记录和总结，仅供个人温习所用，原教程链接[动手学深度学习2.0](https://zh-v2.d2l.ai/)


```python
import torch
from torch.utils import data
import numpy as np
import husky

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels =husky.husky_data.synthetic_data(true_w, true_b, 1000)
```

## 2. 数据集读取

布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据


```python
def load_array(data_arrays, batch_size, is_train=True):
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)#*表示解包元祖成为独立的元素
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项


```python
next(iter(data_iter))
```




    [tensor([[ 1.6764,  0.1199],
             [ 0.1163, -0.5443],
             [-0.1424, -0.5855],
             [ 0.4298,  0.6296],
             [ 0.5119,  1.3102],
             [-1.1822, -0.0264],
             [-0.5096,  1.1026],
             [ 0.2849,  1.3054],
             [ 0.0034,  1.1984],
             [-0.8963,  0.7408]]),
     tensor([[ 7.1323],
             [ 6.2775],
             [ 5.9169],
             [ 2.9078],
             [ 0.7765],
             [ 1.9309],
             [-0.5866],
             [ 0.3293],
             [ 0.1256],
             [-0.1084]])]



## 3. 模型定义

我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入

我们将两个参数传递到nn.Linear中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1


```python
# nn是神经网络的缩写
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))
```

## 4. 模型参数初始化


```python
net[0].weight.data.normal_(0, 0.01)#type:ignore
net[0].bias.data.fill_(0) # type: ignore
```




    tensor([0.])



## 5. 损失函数定义

计算均方误差使用的是MSELoss类，也称为平方$L_2$范数。 默认情况下，它返回所有样本损失的平均值


```python
loss = nn.MSELoss()
```

## 6. 优化算法定义

小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03


```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

## 7. 模型训练


```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

    epoch 1, loss 0.000322
    epoch 2, loss 0.000097
    epoch 3, loss 0.000097
    


```python
w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))#type:ignore
b = net[0].bias.data
print('b的估计误差：', true_b - b)#type:ignore
```

    w的估计误差： tensor([ 0.0004, -0.0008])
    b的估计误差： tensor([0.0004])
    


```python
print(f"y={w.detach().numpy()}x+{b.detach().numpy()}")#type:ignore
```

    y=[[ 1.9995633 -3.3991604]]x+[4.1995535]
    
