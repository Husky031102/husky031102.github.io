---
title: 【深度学习基础】3. 多层感知机 1.多层感知机的基础实现
date: 2023-07-03 00:00:01 +0800
img_path: /assets/img/md
categories: [记录, 深度学习基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
---

## 1. 数据集读取

全文依托于李沐老师的《动手学深度学习》一书，旨在对学习过程中的一些知识点作一点个人的记录和总结，仅供个人温习所用，原教程链接[动手学深度学习2.0](https://zh-v2.d2l.ai/)

通常情况下，我们很难用一个线性的函数来拟合非线性的数据，这时候我们就需要用到多层感知机了，多层感知机就是将多个全连接层按照一定的顺序堆叠在一起，从而实现对非线性数据的拟合，但是很容易发现，如果单纯的使用多个全连接层，那么多层感知机和单层感知机也没有什么区别，因此我们需要在全连接层之间加入激活函数，使其非线性化，从而实现对非线性数据的拟合


```python
import torch
from torch import nn
import husky

batch_size = 256
train_iter, test_iter = husky.husky_data.load_data_fashion_mnist(batch_size)
```

## 2. 参数初始化

我们使用的数据集仍然是Fashion-MNIST，其每个图像由28*28=784个灰度像素值组成，总共可以分类成10种类别，我们设计一个单隐藏层的多层感知机，它将包含256个隐藏单元，一般来说我们设计层的宽度时会使用2的幂次方，这是出于计算机硬件设计考虑的，能够提升计算效率。我们将使用正态分布来初始化权重参数，偏置参数全部初始化为0


```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

## 3. 激活函数定义

作为基础实现，我们基于公式设计一个ReLu函数，而非直接调用框架中的实现


```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

## 4. 模型定义


```python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

## 5. 损失函数定义

在2.线性神经网络 3.softmax回归一节中，我们已经实现过交叉熵损失函数，这里我们直接调用框架中的实现


```python
loss = nn.CrossEntropyLoss(reduction='none')
```

## 6. 模型训练

对于多层感知机，我们使用和softmax回归一样的训练过程，这里我们调用上一节中打包好的训练函数


```python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
husky.husky_train.train_ch2(net, train_iter, test_iter, loss, num_epochs,updater)
```


    
![svg](3.多层感知机 1.多层感知机的基础实现_17_0.svg)
    


## 7. 模型预测


```python
husky.husky_predict.predict_ch2(net, test_iter) 
```


    
![svg](3.多层感知机 1.多层感知机的基础实现_19_0.svg)
    

