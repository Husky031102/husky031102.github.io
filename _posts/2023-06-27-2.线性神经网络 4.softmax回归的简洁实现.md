---
title: 【深度学习基础】2.线性神经网络 4.softmax回归的简洁实现
date: 2023-06-27 00:00:01 +0800
img_path: /assets/img/md
categories: [记录, 深度学习基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
math: true
---
## 1. 数据集读取

全文依托于李沐老师的《动手学深度学习》一书，旨在对学习过程中的一些知识点作一点个人的记录和总结，仅供个人温习所用，原教程链接[动手学深度学习2.0](https://zh-v2.d2l.ai/)


```python
import torch
from torch import nn
import husky

batch_size = 256
train_iter, test_iter = husky.husky_data.load_data_fashion_mnist(batch_size)
```

## 2. 参数初始化

PyTorch不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状

nn.Flatten()将输入图像展平，将形状为(batch_size, 1, 28, 28)的输入转换成形状为(batch_size, 784)的输入，展平操作并不影响批量大小。


```python
# 定义模型
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

# 初始化模型参数
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

#对net的所有模型层应用init_weights函数
net.apply(init_weights)
```




    Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=10, bias=True)
    )



## 3. 损失函数定义

我们采用交叉熵损失函数来作为loss函数


```python
loss = nn.CrossEntropyLoss(reduction='none')
```

## 4. 优化算法定义

优化算法依旧使用小批量随机梯度下降算法  


```python
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

## 5. 模型训练

快速完成基本方法的定义后我们开始训练模型，可以看到，训练的效果和我们自己定义的模型效果是一致的


```python
num_epochs = 10
husky.husky_train.train_ch2(net, train_iter, test_iter, loss, num_epochs, trainer)
```


    
![svg](2.线性神经网络%203.softmax回归的简洁实现_15_0.svg)
    


## 6. 模型预测

接下来简单的跑个预测看看效果


```python
husky.husky_predict.predict_ch2(net, test_iter)
```


    
![svg](2.线性神经网络%203.softmax回归的简洁实现_18_0.svg)
    

