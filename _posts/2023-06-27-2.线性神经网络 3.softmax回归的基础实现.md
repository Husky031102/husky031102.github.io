---
title: 【深度学习基础】2.线性神经网络 3.softmax回归的基础实现
date: 2023-06-27 00:00:00 +0800
img_path: /assets/img/md
categories: [记录, 深度学习基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
---
## 1. 数据集引入

全文依托于李沐老师的《动手学深度学习》一书，旨在对学习过程中的一些知识点作一点个人的记录和总结，仅供个人温习所用，原教程链接[动手学深度学习2.0](https://zh-v2.d2l.ai/)

为了处理分类问题，我们需要输出的是一个概率分布，即对于每个类别的概率预测值。这个概率值是一个非负数，并且所有概率值的和为1。因此，我们需要一个能够输出非负数并且和为1的函数，这个函数就是softmax函数


```python
import torch
from IPython import display
import husky
```

我们首先引入fashion-mnist数据集，这个数据集包含了10个类别的70000个灰度图像，每个图像的高和宽均为28像素。这里的每个图像都相当于一个28*28的矩阵，我们可以将每个矩阵展开成一个长度为784的向量，将每个图像视作784个特征的样本，读取方法可以自行阅读husky_data下load_data_fashion_mnist函数



```python
batch_size = 256
train_iter, test_iter = husky.husky_data.load_data_fashion_mnist(batch_size)
```

## 2. 参数初始化

一如既往，我们需要对参数进行初始化，很显然，对于本次设计的全连接层而言，对应的数学表达式如下，我们的输入$X$是784维的向量，此处我们设计为一行向量，输出$O$则是一个10维的行向量，因此我们选择初始化一个$784*10$的矩阵作为权重$W$，也就是一个784行，每行有10个元素的矩阵，以及一个$1*10$的向量作为偏置，然后我们使用正态分布来初始化权重，使用0来初始化偏置

$$\begin{split}\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}$$


```python
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)#生成一个1*10的全0矩阵  
```

## 3. softmax操作定义

$$
\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.
$$

依照softmax的表达式，我们可以做一个softmax操作的简易实现


```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)#按行求和，并保持列数不变
    return X_exp / partition  # 这里应用了广播机制
```

## 4. 模型定义

定义完softmax操作之后，我们就可以定义模型了，模型的定义很简单，就是一个全连接层，然后在全连接层之后加上softmax操作，这样就可以得到一个概率分布了，因此我们将全连接层传入softmax()操作即可，需要注意的是，我们之前没有对图像做任何处理，因此我们在这里需要将图像reshape成一个行向量，然后再传入模型中


```python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

## 5. 损失函数定义

我们使用交叉熵损失函数来计算损失，交叉熵损失函数的表达式如下，其中$\hat{y}$是预测值，$y$是真实值，$y$是一个one-hot向量，也就是说，如果$y$的第$i$个元素为1，那么这个样本就是第$i$类，否则就不是第$i$类，因此我们可以使用$y$来索引$\hat{y}$，然后计算交叉熵损失

$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$

对于张量的索引，有这样一种写法，比较容易看错，需要注意一下

下面举个例子，我们创建一个数据样本y_hat，其中包含2个样本在3个类别的预测概率， 以及它们对应的真实值，也就是标签y，很显然，第0个样本对应的是第0个标签，第1个样本对应的是第2个标签，因此我们可以使用y来索引y_hat


```python
y = torch.tensor([0, 2])#此处为真实值
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y],y_hat[0,0],y_hat[1,2]#取出y_hat中第0行的第0列和第1行的第2列，也许分开写会更好理解
#也就是取出第0个样本的真实值0对应的预测值，第1个样本的真实值2对应的预测值
#在一次需要取多个指定数据时，可以这样，[[a1,b1,c1],[a2,b2,c2]],取出[a1,a2],[b1,b2],[c1,c2]对应的数
```




    (tensor([0.1000, 0.5000]), tensor(0.1000), tensor(0.5000))




```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
#跟刚刚说的一样，这里取出来的就是[0,y[0]]和[1,y[1]]对应的数，也就是第n个样本的真实值对应的预测值
#很显然对与独热编码来说，非真实值对应的标签的预测值都是0，所以只需要取出真实值对应的预测值即可
cross_entropy(y_hat, y)
```




    tensor([2.3026, 0.6931])



## 6. 分类精度

为了计算精度，我们执行以下操作。 首先，如果y_hat是矩阵，那么假定第一个维度储存的是各个样本，第二个维度存储对应样本的每个类的预测分数。 我们使用argmax获得每行中最大元素的索引来获得预测类别。 然后我们将预测类别与真实y元素进行比较。 由于等式运算符“==”对数据类型很敏感， 因此我们将y_hat的数据类型转换为与y的数据类型一致。 结果是一个包含0（错）和1（对）的张量。 最后，我们求和会得到正确预测的数量


```python
def accuracy(y_hat, y): #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    #从右往左读，先比较y_hat.type(y.dtype) == y，cmp是一个记录0,1的张量
    return float(cmp.type(y.dtype).sum())#返回正确的数量
```

我们也可以简单的计算下正确率，也就是正确预测的数量除以总的样本数量


```python
accuracy(y_hat, y) / len(y)
```




    0.5



为了能够评估精度，我们先定义一个类用于作累加，初始化时传入我们需要累加的变量个数，然后每次调用add()方法时，就会将传入的变量加到累加器上,定义 __getitem__ 方法，使得我们可以直接通过索引访问累加器的值


```python
class Accumulator: #@save
    """在n个变量上累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

接着就是定义一个精度评估函数，用于直接给出精度


```python
def evaluate_accuracy(net, data_iter):#@save
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())#numel()返回元素个数
    return metric[0] / metric[1]
```

由于模型原始权重是随机初始化的，因此这个模型的精度应该接近于随机预测，即0.1


```python
evaluate_accuracy(net, test_iter)
```




    0.0901



## 7. 模型训练

考虑到模型训练过程比较一致，我们用函数整合使其可复用


```python
def train_epoch_ch2(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（线性神经网络）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):#如果使用了内置的优化器
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

处于可视化的目的，我们再定义一个动画类用于动态展示训练过程


```python
class Animator: #@save
    """在动画中绘制数据"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        husky.husky_pic.use_svg_display()
        self.fig, self.axes = husky.husky_pic.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: husky.husky_pic.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()#type:ignore
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)#type:ignore
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

接着依托单次训练函数，我们定义一个训练函数，用于训练多次并生成动画


```python
def train_ch2(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（线性神经网络）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch2(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics#type:ignore
    assert train_loss < 0.5, train_loss#如果训练损失小于0.5则继续往下执行，否则抛出train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc#当训练的精确度在0.7到1之间时继续往下执行，否则抛出train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc#type:ignore
    #当测试的精确度在0.7到1之间时继续往下执行，否则抛出test_acc
```

作为基础实现，我们这里使用自己手写的小批量随机梯度下降函数


```python
lr = 0.1

def updater(batch_size):
    return husky.husky_optimiazaiton.sgd([W, b], lr, batch_size)
```

下面让我们来进行个十轮训练


```python
num_epochs = 10
train_ch2(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```


    
![svg](2.线性神经网络%203.softmax回归的基础实现_45_0.svg)
    


可以看到，精确度在不断提升，损失在不断下降，这是我们预期的结果

## 8. 模型预测

训练完的模型不能只看纸面数据，还要看看实际的工作状态，下面让我们定义一个预测函数用来使用训练好的模型来预测分类，看看效果如何


```python
def predict_ch2(net, test_iter, n=6):  #@save
    """预测标签（线性神经网络）"""
    for X, y in test_iter:#X是图像特征，y是标签
        break#我们只需要一个批量的数据，因此直接跳过即可
    trues = husky.husky_data.get_fashion_mnist_labels(y)#type:ignore
    preds = husky.husky_data.get_fashion_mnist_labels(net(X).argmax(axis=1))#type:ignore
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    husky.husky_pic.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])#type:ignore

predict_ch2(net, test_iter)
```


    
![svg](2.线性神经网络%203.softmax回归的基础实现_49_0.svg)
    

