---
title: 【Pytorch基础】1. 预备 3.数学基础
date: 2023-06-25 00:00:00 +0800
img_path: /assets/img/md
categories: [记录, Pytorch基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
---
需要注意，笔者整理此篇主要是为了让高数基础能够转化为代码实现,因此对于具体的数学概念不会过多赘述，仅仅提及，有不明白的概念可以自行查阅相关资料

## 1. 线性代数

### 1.1. 标量，向量，矩阵和张量

1.1.1. 标量

标量由只有一个元素的张量表示


```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y
```




    (tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))



1.1.2. 向量

向量用一个一维张量表示


```python
x = torch.arange(4)
x
```




    tensor([0, 1, 2, 3])



1.1.2.1. 长度，维度和形状

对于向量，我们既可以像普通数组一样使用len()来访问张量长度


```python
len(x)
```




    4



也可以使用shape属性来访问


```python
x.shape
```




    torch.Size([4])



向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

1.1.3. 矩阵

当我们进一步推广张量的概念时，我们得到了矩阵。 矩阵是具有相同数据类型的二维数组，你可以将矩阵视为一个向量的列表，其中每个向量都是矩阵的一列或一列

$$
\begin{split}\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.\end{split}
$$


```python
A = torch.arange(20).reshape(5, 4)
A
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11],
            [12, 13, 14, 15],
            [16, 17, 18, 19]])



很多时候我们会用到矩阵的转置矩阵，即原矩阵的行和列互换


```python
A.T
```




    tensor([[ 0,  4,  8, 12, 16],
            [ 1,  5,  9, 13, 17],
            [ 2,  6, 10, 14, 18],
            [ 3,  7, 11, 15, 19]])



有部分矩阵，其转置矩阵与原矩阵相等，这样的矩阵被称为对称矩阵


```python
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B,B==B.T
```




    (tensor([[1, 2, 3],
             [2, 0, 4],
             [3, 4, 5]]),
     tensor([[True, True, True],
             [True, True, True],
             [True, True, True]]))



1.1.4. 张量

当我们进一步在维度上推广矩阵的概念时，我们得到了张量。 张量是一个多维数组，其中每个数组都是相同类型的。 张量可以看作是一个矩阵的列表，其中每个矩阵都是相同的。我们通常处理的张量是具有0个轴（标量），1个轴（向量），2个轴（矩阵）或3个轴（立方体）的张量


```python
X = torch.arange(24).reshape(2, 3, 4)
X
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])



### 1.2. 张量运算性质

无论对于标量，向量，矩阵还是更高维度的张量，我们有一些通用的运算性质可以使用

1.2.1. 将两个相同形状的矩阵相加，会在这两个矩阵上按元素执行加法


```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([[ 0.,  2.,  4.,  6.],
             [ 8., 10., 12., 14.],
             [16., 18., 20., 22.],
             [24., 26., 28., 30.],
             [32., 34., 36., 38.]]))



1.2.2. 当我们使用*号乘法时，将执行的是Hadamard积，即两个矩阵的对应元素相乘


```python
A * B
```




    tensor([[  0.,   1.,   4.,   9.],
            [ 16.,  25.,  36.,  49.],
            [ 64.,  81., 100., 121.],
            [144., 169., 196., 225.],
            [256., 289., 324., 361.]])



1.2.3. 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘


```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape#体现其实是一种广播机制
```




    (tensor([[[ 2,  3,  4,  5],
              [ 6,  7,  8,  9],
              [10, 11, 12, 13]],
     
             [[14, 15, 16, 17],
              [18, 19, 20, 21],
              [22, 23, 24, 25]]]),
     torch.Size([2, 3, 4]))



### 1.3. 元素和与降维 

1.3.1. 我们可以通过对所有元素求和来降低张量的维度


```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```




    (tensor([0., 1., 2., 3.]), tensor(6.))



求和不仅仅作用于向量，任意形状的张量均可求元素和


```python
A.shape, A.sum()
```




    (torch.Size([5, 4]), tensor(190.))



默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度


```python
A_sum_axis0 = A.sum(axis=0)# type: ignore
#axis=0表示沿着行的方向进行操作，即对每一列求和
A_sum_axis1 = A.sum(axis=1)# type: ignore
#axis=1表示沿着列的方向进行操作，即对每一行求和
A_sum_axis0, A_sum_axis0.shape,A_sum_axis1, A_sum_axis1.shape
```




    (tensor([40., 45., 50., 55.]),
     torch.Size([4]),
     tensor([ 6., 22., 38., 54., 70.]),
     torch.Size([5]))



也可以通过这种形式来指定多个轴


```python
A.sum(axis=[0, 1])   # type: ignore
# 结果和A.sum()相同
```




    tensor(190.)



除去求和之外，还有一些其他的降维操作，如求平均值，最大值，最小值等


```python
A.mean(axis=0) # type: ignore
```




    tensor([ 8.,  9., 10., 11.])



1.3.2. 有时在调用函数来计算总和或均值时，我们会希望保持轴数不变


```python
sum_A = A.sum(axis=1, keepdims=True) # type: ignore
sum_A
```




    tensor([[ 6.],
            [22.],
            [38.],
            [54.],
            [70.]])



在这种情况下，我们可以指定keepdims = True作为函数的参数，使得计算后仍保持原来的轴数

### 1.4. 向量点积

还有种基本操作叫做点积，也叫做内积。 在数学上，我们使用点积来表示两个向量的乘法。 在深度学习中，我们通常使用点积来表示任意两个张量相同位置元乘积的和


```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y),torch.sum(x * y)#点积等价于元素乘法后求和
```




    (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.), tensor(6.))



### 1.5. 矩阵乘法

我们使用torch.mm()函数来计算俩矩阵的乘法，计算过程遵从我们在线性代数中学习过的规则


```python
B = torch.ones(4, 3)
torch.mm(A, B)
```




    tensor([[ 6.,  6.,  6.],
            [22., 22., 22.],
            [38., 38., 38.],
            [54., 54., 54.],
            [70., 70., 70.]])



对于矩阵-向量积，我们将矩阵视为向量，其中每个元素都是向量的一行，可以用torch.mv()函数来计算


```python
A.shape, x.shape, torch.mv(A, x)
```




    (torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))



### 1.6. 范数

就理解上而言，我们可以将范数视为一个向量有多大的度量。 在机器学习中，我们经常使用范数作为模型参数的正则化项

$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
$$


对于一个普遍的$L_p$范数如上，我们将$L_2$范数称为欧几里得范数，将$L_1$范数称为曼哈顿范数，这也是我们常用到的两种范数


```python
u = torch.tensor([3.0, -4.0])
torch.norm(u),torch.abs(u).sum()#欧几里得范数，曼哈顿范数
```




    (tensor(5.), tensor(7.))



在矩阵中，我们使用Frobenius范数,即矩阵元素的平方和的平方根，其效果相当于将矩阵展平为向量后计算其$L_2$范数

$$
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.
$$


```python
torch.norm(torch.ones((4, 9)))
```




    tensor(6.)



## 2. 微积分

### 2.1 标量变量反向传播

我们以$y=2x^Tx$为例对向量$x$求导



```python
import torch

x = torch.arange(4.0)#给x一个初始值
x
```




    tensor([0., 1., 2., 3.])



首先我们需要一块固定的内存来存储梯度


```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad  # 默认值是None
```

当$x$为一个向量时，$x^Tx$是一个标量，等效于dot(x,x)


```python
y = 2 * torch.dot(x, x)
y
```




    tensor(28., grad_fn=<MulBackward0>)



利用反向传播，我们可以很容易的得到梯度


```python
y.backward()
x.grad
```




    tensor([ 0.,  4.,  8., 12.])



做一个简单的验证，我们可以通过求导公式来验证我们的结果


```python
x.grad == 4 * x
```




    tensor([True, True, True, True])




```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()#type: ignore
y = x.sum()
y.backward()
x.grad
```




    tensor([1., 1., 1., 1.])



### 2.2 非标量变量的反向传播

在PyTorch中有着这样一个规定，不让张量对张量求导，只允许标量对张量求导，因此当$y$是一个向量时，我们需要将$y$先转换为标量


```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的，因此不用设置参数
x.grad.zero_()#type: ignore
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()#先一步转化为标量
x.grad
```




    tensor([0., 2., 4., 6.])



### 2.3 分离计算

有时候我们希望将某些计算移动到记录的计算图之外，我们可以使用detach函数来截断反向传播的计算图，该函数会返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，不同之处只是requirse_grad为false.得到的这个tensir永远不需要计算器梯度，不具有grad，此后即使设置其requirse_grad为true，也不会具有grad

假定我们想要计算$u = x * y$，其中$y$是由另一个计算得到的关于$x$的函数，$u$是我们想要的结果，但我们希望梯度能够反向传播到$y$，但不是$x$，那么我们可以这样做，假定$y=x^2$


```python
x.grad.zero_()#type: ignore
y = x * x
u = y.detach()#截断出一个不具有梯度的u将y看作常数，不参与梯度计算
z = u * x

z.sum().backward()
x.grad == u
```




    tensor([True, True, True, True])



由于记录了$y$的计算结果，我们可以随后在$y$上调用反向传播， 得到$y=x*x$关于的$x$的导数，即$2x$


```python
x.grad.zero_()#type: ignore
y.sum().backward()
x.grad == 2 * x
```




    tensor([True, True, True, True])


