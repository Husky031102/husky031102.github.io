---
title: 【深度学习基础】EX1. 常用汇总 1.激活函数
date: 2023-07-03 00:00:00 +0800
img_path: /assets/img/md
categories: [记录, 深度学习基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
math: true
---
## 1. 常用激活函数

### 1.1. ReLU函数

ReLu函数即Rectified linear unit function，是一种非线性函数，实现简单性能良好，是目前最常用的激活函数之一，简单来说，Relu函数通过将所有负值都设置为零来实现非线性。公式如下：

$$
\operatorname{ReLU}(x) = \max(x, 0).
$$

ReLu函数有许多变体，例如Parameterized ReLU如下，通过给ReLu函数添加一含有参数$\alpha$的线性项，可以使得负值不再都设置为零，而是设置为$\alpha x$，这样可以使得模型更加灵活。

$$\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).$$

### 1.2. sigmoid函数    

对于一个定义域在整个实数域中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function），因为它将实数“挤压”到区间(0, 1)上。sigmoid函数的公式如下：

$$
\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.
$$

sigmoid函数的导数如下： 

$$
\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).
$$

### 1.3. tanh函数

tanh函数是双曲正切函数，它可以将输入变换到区间(-1, 1)上。tanh函数的公式如下：

$$
\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.
$$

tanh函数的导数如下：

$$
\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).
$$
