---
title: 【Pytorch基础】1. 预备 1.数据操作
date: 2023-06-13 00:00:00 +0800
img_path: /assets/img/md
categories: [记录, Pytorch基础]
tags: [记录, 基础学习, Pytorch,深度学习]     # TAG names should always be lowercase
---

## 1. 基础

1.1. 导入torch库


```python
import torch
```

1.2. 利用arange创建一个行向量


```python
x = torch.arange(12)
```

1.3. 访问shape属性来获取张量的形状


```python
x.shape
```




    torch.Size([12])



1.4. 通过reshape函数把行向量变形成张量


```python
X = x.reshape((3, 4))
X
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])



1.4.1 如果不知道张量维度，可以通过-1自动推断


```python
X1 = x.reshape((-1, 4))
X2 = x.reshape((3, -1))
X1, X2
```




    (tensor([[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]]),
     tensor([[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]]))



1.5.全零，全一张量


```python
torch.zeros((2, 3, 4))
```




    tensor([[[0., 0., 0., 0.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]],
    
            [[0., 0., 0., 0.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]]])




```python
torch.ones((2, 3, 4))
```




    tensor([[[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]]])



1.6.随机张量，例如一个满足标准正态分布的3x4张量


```python
torch.randn(3, 4)
```




    tensor([[ 0.6970, -2.1450,  2.3972,  0.0798],
            [ 0.3478, -0.8306, -1.0055,  1.4360],
            [-0.8939, -0.5989, -2.0959,  0.5295]])



1.7. 手动赋值张量


```python
torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6]])
```




    tensor([[1, 2, 3, 4],
            [2, 3, 4, 5],
            [3, 4, 5, 6]])



## 2. 运算

2.1 基本运算


```python
x = torch.tensor([1,2,3,4])
y = torch.tensor([4,3,2,1])
x+y,x-y,x*y,x/y,x**y
```




    (tensor([5, 5, 5, 5]),
     tensor([-3, -1,  1,  3]),
     tensor([4, 6, 6, 4]),
     tensor([0.2500, 0.6667, 1.5000, 4.0000]),
     tensor([1, 8, 9, 4]))



显然，张量的加减乘除运算都是对应元素的运算，除此之外还有很多计算都是对应元素的，例如指数运算等


```python
torch.exp(x)
```




    tensor([ 2.7183,  7.3891, 20.0855, 54.5981])



2.2 张量的拼接

我们依照维度的不同，可以对张量进行不同的拼接操作，例如对于两个矩阵，我们可以在行维度上进行拼接，也可以在列维度上进行拼接

dim=0即0轴，对应的是张量的形状的第一个元素，也就是在行数上来拼接拓展，dim=1即1轴，对应的是张量的形状的第二个元素，也就是在列数上来拼接拓展


```python
X = torch.arange(12, dtype=torch.float32).reshape((3, 4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [ 2.,  1.,  4.,  3.],
             [ 1.,  2.,  3.,  4.],
             [ 4.,  3.,  2.,  1.]]),
     tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
             [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
             [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))



2.3 张量等价判断

对每个对应位置的元素做分别比较，如果相等则返回True，否则返回False，最终反馈为一个布尔型的张量


```python
X == Y
```




    tensor([[False,  True, False,  True],
            [False, False, False, False],
            [False, False, False, False]])



2.4 张量的求和

张量的求和是对张量中所有元素进行求和，得到一个只有一个元素的张量


```python
X.sum()
```




    tensor(66.)



## 3. 广播机制

对于不同形状的张量，我们可以调用广播机制来按元素操作

首先，通过适当的复制元素来拓展张量，使之具有相同的形状

然后，对这两个形状相同的张量按元素操作


```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```




    (tensor([[0],
             [1],
             [2]]),
     tensor([[0, 1]]))




```python
a + b
```




    tensor([[0, 1],
            [1, 2],
            [2, 3]])



其实际工作变化如下，a,b分别通过自我复制来拓展至相同形状，然后按元素操作


```python
a =torch.cat((a,a),dim= 1)
b = torch.cat((b,b,b),dim= 0)
a,b
```




    (tensor([[0, 0],
             [1, 1],
             [2, 2]]),
     tensor([[0, 1],
             [0, 1],
             [0, 1]]))



## 4. 索引与切片

张量的索引和切片类似数组


```python
X,X[-1],X[2:3]#X[-1]是最后一个元素，X[2:3]是第三行，别忘记了python的切片是左闭右开的
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.]]),
     tensor([ 8.,  9., 10., 11.]),
     tensor([[ 8.,  9., 10., 11.]]))



当然也可以直接通过索引来修改张量


```python
X[2,3] = 233
X
```




    tensor([[  0.,   1.,   2.,   3.],
            [  4.,   5.,   6.,   7.],
            [  8.,   9.,  10., 233.]])




```python
X[0:2, :] = 666
X
```




    tensor([[666., 666., 666., 666.],
            [666., 666., 666., 666.],
            [  8.,   9.,  10., 233.]])



## 5. 内存优化

当我们给一个变量赋值时，例如y=y+x，我们实际上是在内存中开辟了一个新的空间来存储新的y，而不是在原来的y的内存空间上进行修改  
当我们对大规模的数据进行操作时，这样的操作会导致内存的浪费，因此我们可以通过索引来进行操作，例如y[:]=y+x，这样就不会开辟新的内存空间，而是直接在原来的内存空间上进行修改


```python
old = id(Y)
Y = Y + X
id(Y) == old # False，说明重新开辟了内存空间
```




    False




```python
old = id(Y)
Y[:] = X + Y
id(Y) == old # True，说明没有开辟新的内存空间
```




    True



还有一种写法也是可行的


```python
old = id(Y)
Y += X
id(Y) == old # True，说明没有开辟新的内存空间
```




    True



## 6. 与其他对象间的互相转换

numpy是另一个常用的科学计算库，它和pytorch类似，也是处理多维数组的，我们可以很轻易的将numpy的数组转换成pytorch的张量，反之亦然


```python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```




    (numpy.ndarray, torch.Tensor)



对于单个元素的张量，我们可以用item函数来将其转换成python标量，也可以用python内置的类型转化


```python
a = torch.tensor([11.2])
a, a.item(), float(a), int(a)
```




    (tensor([11.2000]), 11.199999809265137, 11.199999809265137, 11)



你可能发现，转化过来的标量不等于我原先设定的数，这个其实是因为浮点数的精度问题，不是所有的十进制小数都可以精确的表示为一个二进制小数  
最经典的莫过于下面的0.1+0.2！=0.3，我们有时候需要特别注意到这一点


```python
0.1+0.2 == 0.3,0.1+0.2
```




    (False, 0.30000000000000004)


